# Version 03 Validators

Generated Oracle-based ideal responses and validators for the split-prompt architecture.

## Files Created

### Ideal Responses (Golden Records)
- `facts-03-ideal.json` - 17 facts from introduction conversation
- `traits-03-ideal.json` - 8 traits (communication style, personality patterns)
- `topics-03-ideal.json` - 4 topics (responsibilities, stories, conflicts)
- `people-03-ideal.json` - 6 people (wife, kids, brother, parents)

### Validators
- `facts-03.ts` - Validates fact extraction with scoring
- `traits-03.ts` - Validates trait extraction with scoring
- `topics-03.ts` - Validates topic extraction with scoring
- `people-03.ts` - Validates people extraction with scoring

## Usage

```bash
# Facts
npm run bench -- \
  --system tests/model/prompts/fastScan/system_03_fact.md \
  --user tests/model/prompts/fastScan/user_03_fact.md \
  --model local:qwen/qwen3-30b-a3b-2507 \
  --runs 20 \
  --name facts-baseline \
  --validator tests/model/validators/facts-03.ts

# Traits
npm run bench -- \
  --system tests/model/prompts/fastScan/system_03_trait.md \
  --user tests/model/prompts/fastScan/user_03_trait.md \
  --model local:qwen/qwen3-30b-a3b-2507 \
  --runs 20 \
  --name traits-baseline \
  --validator tests/model/validators/traits-03.ts

# Topics
npm run bench -- \
  --system tests/model/prompts/fastScan/system_03_topic.md \
  --user tests/model/prompts/fastScan/user_03_topic.md \
  --model local:qwen/qwen3-30b-a3b-2507 \
  --runs 20 \
  --name topics-baseline \
  --validator tests/model/validators/topics-03.ts

# People
npm run bench -- \
  --system tests/model/prompts/fastScan/System_03_people.md \
  --user tests/model/prompts/fastScan/user_03_people.md \
  --model local:qwen/qwen3-30b-a3b-2507 \
  --runs 20 \
  --name people-baseline \
  --validator tests/model/validators/people-03.ts
```

## Scoring

All validators use the same scoring formula:
- **40% Recall**: Items found / Total ideal items
- **30% Precision**: Valid items / Total response items
- **20% Type accuracy**: Correct categorization
- **10% Confidence accuracy**: Appropriate confidence levels
- **-5% per hallucination**: Penalty for fabricated items

## Schema Changes (v03)

**Old** (v01 - combined):
```json
{
  "name": "Self-deprecating humor",
  "type": "trait",
  "confidence": "high"
}
```

**New** (v03 - split):
```json
{
  "type_of_trait": "Communication Style",
  "value_of_trait": "Self-deprecating",
  "confidence": "high",
  "reason": "Makes light of personal appearance..."
}
```

The split allows models to categorize naturally instead of fighting their tendency to sub-categorize.

## Oracle Generation

These ideal responses were generated by Claude Opus 4.5 analyzing the prompts and conversation. No markdown fences (as requested), extracted cleanly into JSON files.

## Next Steps

1. Run baseline benchmarks for all four types
2. Compare which types have the worst scores
3. Iterate on prompts for problematic types
4. Use `npm run analyze` to compare results across runs
